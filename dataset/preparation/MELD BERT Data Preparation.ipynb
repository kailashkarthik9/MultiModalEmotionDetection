{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, using the CPU instead.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMOTION_IDS = {\n",
    "    'anger/disgust': 0,\n",
    "    'fear/surprise': 1,\n",
    "    'happiness': 2,\n",
    "    'neutral': 3,\n",
    "    'sadness': 4\n",
    "}\n",
    "\n",
    "MELD_EMOTION_MAP = {\n",
    "    'joy': 'happiness',\n",
    "    'sadness': 'sadness',\n",
    "    'surprise': 'fear/surprise',\n",
    "    'fear': 'fear/surprise',\n",
    "    'anger': 'anger/disgust',\n",
    "    'disgust': 'anger/disgust',\n",
    "    'neutral': 'neutral'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training sentences: 9,989\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Utterance</th>\n",
       "      <th>Speaker</th>\n",
       "      <th>Emotion</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Dialogue_ID</th>\n",
       "      <th>Utterance_ID</th>\n",
       "      <th>Season</th>\n",
       "      <th>Episode</th>\n",
       "      <th>StartTime</th>\n",
       "      <th>EndTime</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sr No.</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>344</th>\n",
       "      <td>Y'know, last night was embarrassing for you too.</td>\n",
       "      <td>Ross</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>34</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>23</td>\n",
       "      <td>00:14:43,132</td>\n",
       "      <td>00:14:46,259</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3708</th>\n",
       "      <td>Im an alien. Im an alien.</td>\n",
       "      <td>Chandler</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>370</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>00:22:29,598</td>\n",
       "      <td>00:22:31,766</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4015</th>\n",
       "      <td>No, no, no... why, because it might get weird ...</td>\n",
       "      <td>Ross</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>410</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>00:19:41,430</td>\n",
       "      <td>00:19:43,347</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3566</th>\n",
       "      <td>Went down to the docks. Bet ya didn't know you...</td>\n",
       "      <td>Monica</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>356</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>00:01:19,496</td>\n",
       "      <td>00:01:24,917</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9819</th>\n",
       "      <td>I mean doesn't she have any y'know other strip...</td>\n",
       "      <td>Rachel</td>\n",
       "      <td>anger</td>\n",
       "      <td>negative</td>\n",
       "      <td>977</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>00:21:10,269</td>\n",
       "      <td>00:21:13,813</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3301</th>\n",
       "      <td>Then keep running.</td>\n",
       "      <td>Phoebe</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>329</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>00:13:38,567</td>\n",
       "      <td>00:13:39,692</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3729</th>\n",
       "      <td>Oh. They don't.</td>\n",
       "      <td>Roger</td>\n",
       "      <td>surprise</td>\n",
       "      <td>positive</td>\n",
       "      <td>372</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>00:19:29,376</td>\n",
       "      <td>00:19:31,752</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>Th-th-that's all it is, a third nipple.</td>\n",
       "      <td>Ross</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>23</td>\n",
       "      <td>00:05:30,246</td>\n",
       "      <td>00:05:32,122</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>Man, I didn't think we were gonna make it!</td>\n",
       "      <td>Joey</td>\n",
       "      <td>surprise</td>\n",
       "      <td>positive</td>\n",
       "      <td>18</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>21</td>\n",
       "      <td>00:05:39,047</td>\n",
       "      <td>00:05:41,548</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8147</th>\n",
       "      <td>Thats it?! I gave up my</td>\n",
       "      <td>Chandler</td>\n",
       "      <td>joy</td>\n",
       "      <td>positive</td>\n",
       "      <td>819</td>\n",
       "      <td>17</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>00:07:02,672</td>\n",
       "      <td>00:07:05,757</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Utterance   Speaker   Emotion  \\\n",
       "Sr No.                                                                          \n",
       "344      Y'know, last night was embarrassing for you too.      Ross   neutral   \n",
       "3708                          Im an alien. Im an alien.  Chandler   neutral   \n",
       "4015    No, no, no... why, because it might get weird ...      Ross   neutral   \n",
       "3566    Went down to the docks. Bet ya didn't know you...    Monica   neutral   \n",
       "9819    I mean doesn't she have any y'know other strip...    Rachel     anger   \n",
       "3301                                   Then keep running.    Phoebe   neutral   \n",
       "3729                                      Oh. They don't.     Roger  surprise   \n",
       "996               Th-th-that's all it is, a third nipple.      Ross   neutral   \n",
       "199            Man, I didn't think we were gonna make it!      Joey  surprise   \n",
       "8147                             Thats it?! I gave up my  Chandler       joy   \n",
       "\n",
       "       Sentiment  Dialogue_ID  Utterance_ID  Season  Episode     StartTime  \\\n",
       "Sr No.                                                                       \n",
       "344      neutral           34             5       5       23  00:14:43,132   \n",
       "3708     neutral          370             2       3       10  00:22:29,598   \n",
       "4015     neutral          410            11       1       24  00:19:41,430   \n",
       "3566     neutral          356             2       3        3  00:01:19,496   \n",
       "9819    negative          977             4       3       12  00:21:10,269   \n",
       "3301     neutral          329            10       6        6  00:13:38,567   \n",
       "3729    positive          372             7       1       13  00:19:29,376   \n",
       "996      neutral          100             0       3       23  00:05:30,246   \n",
       "199     positive           18             9       5       21  00:05:39,047   \n",
       "8147    positive          819            17       7        4  00:07:02,672   \n",
       "\n",
       "             EndTime  Label  \n",
       "Sr No.                       \n",
       "344     00:14:46,259      3  \n",
       "3708    00:22:31,766      3  \n",
       "4015    00:19:43,347      3  \n",
       "3566    00:01:24,917      3  \n",
       "9819    00:21:13,813      0  \n",
       "3301    00:13:39,692      3  \n",
       "3729    00:19:31,752      1  \n",
       "996     00:05:32,122      3  \n",
       "199     00:05:41,548      1  \n",
       "8147    00:07:05,757      2  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset into a pandas dataframe.\n",
    "df_train = pd.read_csv(\"data/train_sent_emo.csv\", index_col=0)\n",
    "df_test = pd.read_csv(\"data/test_sent_emo.csv\", index_col=0)\n",
    "df_val = pd.read_csv(\"data/dev_sent_emo.csv\", index_col=0)\n",
    "\n",
    "df_train['Label'] = df_train['Emotion'].apply(lambda emotion: EMOTION_IDS[MELD_EMOTION_MAP[emotion]])\n",
    "df_test['Label'] = df_test['Emotion'].apply(lambda emotion: EMOTION_IDS[MELD_EMOTION_MAP[emotion]])\n",
    "df_val['Label'] = df_val['Emotion'].apply(lambda emotion: EMOTION_IDS[MELD_EMOTION_MAP[emotion]])\n",
    "\n",
    "# Report the number of sentences.\n",
    "print('Number of training sentences: {:,}\\n'.format(df_train.shape[0]))\n",
    "\n",
    "# Display 10 random rows from the data.\n",
    "df_train.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences = df_train.Utterance.values\n",
    "train_labels = df_train.Label.values\n",
    "test_sentences = df_test.Utterance.values\n",
    "test_labels = df_test.Label.values\n",
    "val_sentences = df_val.Utterance.values\n",
    "val_labels = df_val.Label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT tokenizer...\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load the BERT tokenizer.\n",
    "print('Loading BERT tokenizer...')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Original:  also I was the point person on my companys transition from the KL-5 to GR-6 system.\n",
      "Tokenized:  ['also', 'i', 'was', 'the', 'point', 'person', 'on', 'my', '[UNK]', 'transition', 'from', 'the', 'k', '##l', '-', '5', 'to', 'gr', '-', '6', 'system', '.']\n",
      "Token IDs:  [2036, 1045, 2001, 1996, 2391, 2711, 2006, 2026, 100, 6653, 2013, 1996, 1047, 2140, 1011, 1019, 2000, 24665, 1011, 1020, 2291, 1012]\n"
     ]
    }
   ],
   "source": [
    "# Print the original sentence.\n",
    "print(' Original: ', train_sentences[0])\n",
    "\n",
    "# Print the sentence split into tokens.\n",
    "print('Tokenized: ', tokenizer.tokenize(train_sentences[0]))\n",
    "\n",
    "# Print the sentence mapped to token ids.\n",
    "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(train_sentences[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max train sentence length:  95\n",
      "Max test sentence length:  61\n",
      "Max val sentence length:  49\n"
     ]
    }
   ],
   "source": [
    "train_max_len = 0\n",
    "for sent in train_sentences:\n",
    "    input_ids = tokenizer.encode(sent, add_special_tokens=True)\n",
    "    train_max_len = max(train_max_len, len(input_ids))\n",
    "print('Max train sentence length: ', train_max_len)\n",
    "\n",
    "test_max_len = 0\n",
    "for sent in test_sentences:\n",
    "    input_ids = tokenizer.encode(sent, add_special_tokens=True)\n",
    "    test_max_len = max(test_max_len, len(input_ids))\n",
    "print('Max test sentence length: ', test_max_len)\n",
    "\n",
    "val_max_len = 0\n",
    "for sent in val_sentences:\n",
    "    input_ids = tokenizer.encode(sent, add_special_tokens=True)\n",
    "    val_max_len = max(val_max_len, len(input_ids))\n",
    "print('Max val sentence length: ', val_max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_ids = []\n",
    "train_attention_masks = []\n",
    "for sent in train_sentences:\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 64,           # Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                        truncation=True\n",
    "                   )\n",
    "    train_input_ids.append(encoded_dict['input_ids'])\n",
    "    train_attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "train_input_ids = torch.cat(train_input_ids, dim=0)\n",
    "train_attention_masks = torch.cat(train_attention_masks, dim=0)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "\n",
    "test_input_ids = []\n",
    "test_attention_masks = []\n",
    "for sent in test_sentences:\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 64,           # Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                        truncation=True\n",
    "                   )\n",
    "    test_input_ids.append(encoded_dict['input_ids'])\n",
    "    test_attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "test_input_ids = torch.cat(test_input_ids, dim=0)\n",
    "test_attention_masks = torch.cat(test_attention_masks, dim=0)\n",
    "test_labels = torch.tensor(test_labels)\n",
    "\n",
    "val_input_ids = []\n",
    "val_attention_masks = []\n",
    "for sent in val_sentences:\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = 64,           # Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                        truncation=True\n",
    "                   )\n",
    "    val_input_ids.append(encoded_dict['input_ids'])\n",
    "    val_attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "val_input_ids = torch.cat(val_input_ids, dim=0)\n",
    "val_attention_masks = torch.cat(val_attention_masks, dim=0)\n",
    "val_labels = torch.tensor(val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, random_split\n",
    "\n",
    "train_dataset = TensorDataset(train_input_ids, train_attention_masks, train_labels)\n",
    "test_dataset = TensorDataset(test_input_ids, test_attention_masks, test_labels)\n",
    "val_dataset = TensorDataset(val_input_ids, val_attention_masks, val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(train_dataset, 'train_dataset.pt')\n",
    "torch.save(test_dataset, 'test_dataset.pt')\n",
    "torch.save(val_dataset, 'val_dataset.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1109"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36] *",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
